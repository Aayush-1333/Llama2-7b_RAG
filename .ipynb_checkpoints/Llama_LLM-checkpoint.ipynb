{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e2b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index import ServiceContext, SimpleDirectoryReader, \\\n",
    "VectorStoreIndex, get_response_synthesizer, load_index_from_storage, set_global_service_context\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor\n",
    "from llama_index.response.pprint_utils import pprint_response\n",
    "\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc173e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c184840a204f448733f38d00d13557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    query_wrapper_prompt=PromptTemplate(\"<s> [INST] {query_str} [/INST]\"),\n",
    "    context_window=3900,\n",
    "    model_kwargs={\n",
    "        \"token\": HF_TOKEN,\n",
    "        \"quantization_config\": quantization_config\n",
    "    },\n",
    "    tokenizer_kwargs={\n",
    "        \"token\": HF_TOKEN\n",
    "    },\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77dd1d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\",\n",
    "    model_kwargs={\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    ")\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=2048, chunk_overlap=30)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, \n",
    "                                               text_splitter=text_splitter)\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d69b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for existence of persistent vector_store\n",
    "if os.path.exists('vector_store_data'):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir='vector_store_data')\n",
    "    vector_index = load_index_from_storage(storage_context=storage_context)\n",
    "else:\n",
    "    docs = SimpleDirectoryReader(\"Data/\").load_data()   # read the data from the folder\n",
    "    # Creating persistent client\n",
    "    db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    # create collection\n",
    "    chroma_collection = db.get_or_create_collection(\"vectorDB\")\n",
    "\n",
    "    # assign chroma store as vector_store to the context\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    # creating index\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        docs,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    # storing index to disk\n",
    "    vector_index.storage_context.persist(persist_dir='vector_store_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82cd21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# configure node postproceesors\n",
    "s_processor = SimilarityPostprocessor(similarity_cutoff=0.79)\n",
    "k_processor = KeywordNodePostprocessor(\n",
    "    exclude_keywords=[\"environmental\"]\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(service_context=service_context)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[s_processor, k_processor],\n",
    "    response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6787e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact\n",
    "# query_engine = vector_index.as_query_engine(response_mode=\"compact\")\n",
    "# response = query_engine.query(\"Is this book worth reading?\")\n",
    "\n",
    "# pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244e0abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (press q to exit) > What is python\n",
      "\n",
      "Final Response: Based on the provided context, it appears that the Python code is creating a service instance for a\n",
      "\"geoPysnmp\" application. The code imports the necessary Python modules and defines a service decorator, which is\n",
      "required for creating a service instance. The code then sets up a variable \"geoVars\" and initializes it with key/value\n",
      "pairs from an XML template.  The code then reads the \"local time zone\" from the service YANG model and reads data from\n",
      "the GeoCatalog to obtain values appropriate to the \"localTZ\". Finally, the code assigns values read from the GeoCatalog\n",
      "to variables that will be passed to the XML template when it is applied.  In summary, the Python code is creating a\n",
      "service instance for a \"geoPysnmp\" application, setting up variables for the service and reading data from the\n",
      "GeoCatalog to obtain values appropriate to the \"localTZ\".  To further refine the answer, the original query asked what\n",
      "the Python code is doing, and the provided context is the Cisco dCloud documentation for a \"Service callbacks\" example.\n",
      "The context does not provide any additional information about the Python code, so the original answer remains unchanged\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "User (press q to exit) > q\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# refine\n",
    "query_engine = vector_index.as_query_engine(response_mode=\"refine\")\n",
    "while True:\n",
    "    user_query = input(\"User (press q to exit) > \")\n",
    "    print()\n",
    "    if user_query.casefold() == 'q':\n",
    "        break\n",
    "    response = query_engine.query(user_query)\n",
    "    pprint_response(response, wrap_width=120)\n",
    "    print('-' * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_summarize\n",
    "# query_engine = vector_index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "# response = query_engine.query(\"Is this book worth reading?\")\n",
    "\n",
    "# pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat engine\n",
    "# chat_engine = vector_index.as_chat_engine()\n",
    "# while True:\n",
    "#     user_query = input(\"User (press q to exit) > \")\n",
    "#     print()\n",
    "#     if user_query.casefold() == 'q':\n",
    "#         break\n",
    "    \n",
    "#     response = chat_engine.chat(user_query)\n",
    "#     pprint_response(response, wrap_width=120)\n",
    "#     print(\"-\" * 100)\n",
    "#     print()\n",
    "\n",
    "# chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db11974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
